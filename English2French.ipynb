{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importing needed dependencies :\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "from string import digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# read txtfile into a list line by line :\n",
    "with open('fra-eng/fra.txt', 'r', encoding='utf-8-sig') as f:\n",
    "    text = f.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Character level processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define impty lists to store the samples in them:\n",
    "en_samples = []\n",
    "de_samples = []\n",
    "\n",
    "# Define impty sets to store the characters in them:\n",
    "en_chars = set()\n",
    "de_chars = set()\n",
    "\n",
    "# Split the samples and get the character sets :\n",
    "for line in text:\n",
    "    en_ , de_ = line.split('\\t')\n",
    "    de_ = '\\t' + de_\n",
    "    for char in de_:\n",
    "        if char not in de_chars:\n",
    "            de_chars.add(char)\n",
    "    for char in en_:\n",
    "        if char not in en_chars:\n",
    "            en_chars.add(char)\n",
    "    en_samples.append(en_)\n",
    "    de_samples.append(de_)\n",
    "   \n",
    "# Add the chars \\t and \\n to the sets \n",
    "de_chars.add('\\n')\n",
    "de_chars.add('\\t')\n",
    "\n",
    "en_chars.add('\\n')\n",
    "en_chars.add('\\t') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "’\n"
     ]
    }
   ],
   "source": [
    "print(next(iter(de_chars)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make the needed dictionaries to convert characters to integers and the opposite : \n",
    "de_char_to_int = dict()\n",
    "de_int_to_char = dict()\n",
    "en_char_to_int = dict()\n",
    "en_int_to_char = dict()\n",
    "for i,char in enumerate(de_chars):\n",
    "    de_char_to_int[char] = i\n",
    "    de_int_to_char[i]    = char\n",
    "    \n",
    "for i,char in enumerate(en_chars):    \n",
    "    en_char_to_int[char] = i\n",
    "    en_int_to_char[i]    = char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of E Samples  \t: 167130\n",
      "Number of D Samples \t: 167130\n",
      "Number of D Chars  \t: 115\n",
      "Number of E Chars \t: 93\n",
      "The Longest D Sample has 351 Chars\n",
      "The Longest E Sample has 286 Chars\n"
     ]
    }
   ],
   "source": [
    "# get lengths and sizes :\n",
    "num_en_chars = len(en_chars)\n",
    "num_de_chars = len(de_chars)\n",
    "\n",
    "max_en_chars_per_sample = max([len(sample) for sample in en_samples])\n",
    "max_de_chars_per_sample = max([len(sample) for sample in de_samples])\n",
    "\n",
    "num_en_samples = len(en_samples)\n",
    "num_de_samples = len(de_samples)\n",
    "\n",
    "print(f'Number of E Samples  \\t: {len(de_samples)}')\n",
    "print(f'Number of D Samples \\t: {len(en_samples)}')\n",
    "\n",
    "print(f'Number of D Chars  \\t: {len(de_chars)}')\n",
    "print(f'Number of E Chars \\t: {len(en_chars)}')\n",
    "\n",
    "print(f'The Longest D Sample has {max_de_chars_per_sample} Chars')\n",
    "print(f'The Longest E Sample has {max_en_chars_per_sample} Chars')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initiate numpy arrays to hold the data that our seq2seq model will use:\n",
    "encoder_input_data = np.zeros((num_en_samples,\n",
    "                               max_en_chars_per_sample,\n",
    "                               num_en_chars),\n",
    "                              dtype='float32')\n",
    "\n",
    "decoder_input_data = np.zeros((num_de_samples,\n",
    "                               max_de_chars_per_sample,\n",
    "                               num_de_chars),\n",
    "                              dtype='float32')\n",
    "\n",
    "target_data = np.zeros((num_de_samples,\n",
    "                       max_de_chars_per_sample,\n",
    "                       num_de_chars),\n",
    "                      dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of encoder_input_data : (167130, 286, 93)\n",
      "Shape of decoder_input_data : (167130, 351, 115)\n",
      "Shape of target_data        : (167130, 351, 115)\n"
     ]
    }
   ],
   "source": [
    "for i, (en_sample, de_sample) in enumerate(zip(en_samples, de_samples)):\n",
    "    for char, en_char in enumerate(en_sample):\n",
    "        encoder_input_data[i, char, en_char_to_int[en_char]] = 1\n",
    "    for char, de_char in enumerate(de_sample):\n",
    "        decoder_input_data[i, char, de_char_to_int[de_char]] = 1\n",
    "        if char > 0 :\n",
    "            target_data[i, char-1, de_char_to_int[de_char]]  = 1\n",
    "            \n",
    "print(f'Shape of encoder_input_data : {encoder_input_data.shape}')\n",
    "print(f'Shape of decoder_input_data : {decoder_input_data.shape}')\n",
    "print(f'Shape of target_data        : {target_data.shape}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word level processing (using embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read txtfile  :\n",
    "file_name = 'fra-eng/fra.txt'\n",
    "lines = pd.read_table(file_name, names=['en', 'de'], encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     en        de\n",
      "0   Go.      Va !\n",
      "1   Hi.   Salut !\n",
      "2  Run!   Cours !\n",
      "3  Run!  Courez !\n",
      "4  Who?     Qui ?\n"
     ]
    }
   ],
   "source": [
    "print(lines.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert text to lowercase :\n",
    "lines.en=lines.en.apply(lambda x: x.lower())\n",
    "lines.de=lines.de.apply(lambda x: x.lower())\n",
    "\n",
    "# Process commas :\n",
    "lines.en=lines.en.apply(lambda x: re.sub(\"'\", '', x)).apply(lambda x: re.sub(\",\", ' COMMA', x))\n",
    "lines.de=lines.de.apply(lambda x: re.sub(\"'\", '', x)).apply(lambda x: re.sub(\",\", ' COMMA', x))\n",
    "\n",
    "# Getting rid of punctuation\n",
    "exclude = set(string.punctuation)\n",
    "lines.en=lines.en.apply(lambda x: ''.join(ch for ch in x if ch not in exclude))\n",
    "lines.de=lines.de.apply(lambda x: ''.join(ch for ch in x if ch not in exclude))\n",
    "\n",
    "# Getting rid of digits\n",
    "remove_digits = str.maketrans('', '', digits)\n",
    "lines.en=lines.en.apply(lambda x: x.translate(remove_digits))\n",
    "lines.de=lines.de.apply(lambda x: x.translate(remove_digits))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample Processing :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of En samples  \t: 167130\n",
      "Number of De samples  \t: 167130\n",
      "Max number of En words per sample  \t: 54\n",
      "Max number of De words per sample  \t: 63\n",
      "Number of En words  \t: 14234\n",
      "Number of De words  \t: 28948\n"
     ]
    }
   ],
   "source": [
    "# Appending SOS and EOS to target data : \n",
    "lines.de = lines.de.apply(lambda x : 'SOS_ '+ x + ' _EOS')\n",
    "\n",
    "# Create word dictionaries :\n",
    "en_words=set()\n",
    "for line in lines.en:\n",
    "    for word in line.split():\n",
    "        if word not in en_words:\n",
    "            en_words.add(word)\n",
    "    \n",
    "de_words=set()\n",
    "for line in lines.de:\n",
    "    for word in line.split():\n",
    "        if word not in de_words:\n",
    "            de_words.add(word)\n",
    "            \n",
    "# get lengths and sizes :\n",
    "num_en_words = len(en_words)\n",
    "num_de_words = len(de_words)\n",
    "\n",
    "max_en_words_per_sample = max([len(sample.split()) for sample in lines.en])+5\n",
    "max_de_words_per_sample = max([len(sample.split()) for sample in lines.de])+5\n",
    "\n",
    "num_en_samples = len(lines.en)\n",
    "num_de_samples = len(lines.de)\n",
    "\n",
    "print(f'Number of En samples  \\t: {num_en_samples}')\n",
    "print(f'Number of De samples  \\t: {num_de_samples}')\n",
    "print(f'Max number of En words per sample  \\t: {max_en_words_per_sample}')\n",
    "print(f'Max number of De words per sample  \\t: {max_de_words_per_sample}')\n",
    "print(f'Number of En words  \\t: {num_en_words}')\n",
    "print(f'Number of De words  \\t: {num_de_words}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get lists of words :\n",
    "input_words = sorted(list(en_words))\n",
    "target_words = sorted(list(de_words))\n",
    "\n",
    "en_token_to_int = dict()\n",
    "en_int_to_token = dict()\n",
    "\n",
    "de_token_to_int = dict()\n",
    "de_int_to_token = dict()\n",
    "\n",
    "#Tokenizing the words ( Convert them to numbers ) :\n",
    "for i,token in enumerate(input_words):\n",
    "    en_token_to_int[token] = i\n",
    "    en_int_to_token[i]     = token\n",
    "\n",
    "for i,token in enumerate(target_words):\n",
    "    de_token_to_int[token] = i\n",
    "    de_int_to_token[i]     = token\n",
    "\n",
    "# initiate numpy arrays to hold the data that our seq2seq model will use:\n",
    "encoder_input_data = np.zeros(\n",
    "    (num_en_samples, max_en_words_per_sample),\n",
    "    dtype='float32')\n",
    "decoder_input_data = np.zeros(\n",
    "    (num_de_samples, max_de_words_per_sample),\n",
    "    dtype='float32')\n",
    "decoder_target_data = np.zeros(\n",
    "    (num_de_samples, max_de_words_per_sample, num_de_words),\n",
    "    dtype='float32')\n",
    "\n",
    "# Process samples, to get input, output, target data:\n",
    "for i, (input_text, target_text) in enumerate(zip(lines.en, lines.de)):\n",
    "    for t, word in enumerate(input_text.split()):\n",
    "        encoder_input_data[i, t] = en_token_to_int[word]\n",
    "    for t, word in enumerate(target_text.split()):\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        decoder_input_data[i, t] = de_token_to_int[word]\n",
    "        if t > 0:\n",
    "            # decoder_target_data will be ahead by one timestep\n",
    "            # and will not include the start character.\n",
    "            decoder_target_data[i, t - 1, de_token_to_int[word]] = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Input, Embedding, TimeDistributed, Flatten, Dropout\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import LSTM as lstm\n",
    "# from keras.layers import CuDNNLSTM as lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/muralikorrapati/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/muralikorrapati/anaconda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "# Defining some constants: \n",
    "vec_len       = 300   # Length of the vector that we willl get from the embedding layer\n",
    "latent_dim    = 1024  # Hidden layers dimension \n",
    "dropout_rate  = 0.2   # Rate of the dropout layers\n",
    "batch_size    = 64    # Batch size\n",
    "epochs        = 30    # Number of epochs\n",
    "\n",
    "# Define an input sequence and process it.\n",
    "# Input layer of the encoder :\n",
    "encoder_input = Input(shape=(None,))\n",
    "\n",
    "# Hidden layers of the encoder :\n",
    "encoder_embedding = Embedding(input_dim = num_en_words, output_dim = vec_len)(encoder_input)\n",
    "encoder_dropout   = (TimeDistributed(Dropout(rate = dropout_rate)))(encoder_embedding)\n",
    "encoder_LSTM      = lstm(latent_dim, return_sequences=True)(encoder_dropout)\n",
    "\n",
    "# Output layer of the encoder :\n",
    "encoder_LSTM2_layer = lstm(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_LSTM2_layer(encoder_LSTM)\n",
    "\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "# Input layer of the decoder :\n",
    "decoder_input = Input(shape=(None,))\n",
    "\n",
    "# Hidden layers of the decoder :\n",
    "decoder_embedding_layer = Embedding(input_dim = num_de_words, output_dim = vec_len)\n",
    "decoder_embedding = decoder_embedding_layer(decoder_input)\n",
    "\n",
    "decoder_dropout_layer = (TimeDistributed(Dropout(rate = dropout_rate)))\n",
    "decoder_dropout = decoder_dropout_layer(decoder_embedding)\n",
    "\n",
    "decoder_LSTM_layer = lstm(latent_dim, return_sequences=True)\n",
    "decoder_LSTM = decoder_LSTM_layer(decoder_dropout, initial_state = encoder_states)\n",
    "\n",
    "decoder_LSTM_2_layer = lstm(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_LSTM_2,_,_ = decoder_LSTM_2_layer(decoder_LSTM)\n",
    "\n",
    "# Output layer of the decoder :\n",
    "decoder_dense = Dense(num_de_words, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_LSTM_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 300)    4270200     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, None, 300)    0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, None, 300)    8684400     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, None, 1024)   5427200     time_distributed_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_2 (TimeDistrib (None, None, 300)    0           embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, 1024), (None 8392704     lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   (None, None, 1024)   5427200     time_distributed_2[0][0]         \n",
      "                                                                 lstm_2[0][1]                     \n",
      "                                                                 lstm_2[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_4 (LSTM)                   [(None, None, 1024), 8392704     lstm_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 28948)  29671700    lstm_4[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 70,266,108\n",
      "Trainable params: 70,266,108\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_input, decoder_input], decoder_outputs)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Define a checkpoint callback :\n",
    "checkpoint_name = 'Weights-{epoch:03d}--{val_loss:.5f}.hdf5' \n",
    "checkpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 1, save_best_only = True, mode ='auto')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/muralikorrapati/anaconda/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /Users/muralikorrapati/anaconda/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Train on 4600 samples, validate on 400 samples\n",
      "Epoch 1/30\n"
     ]
    }
   ],
   "source": [
    "num_train_samples = 5000\n",
    "# Run training\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "model.fit([encoder_input_data[:num_train_samples,:],\n",
    "               decoder_input_data[:num_train_samples,:]],\n",
    "               decoder_target_data[:num_train_samples,:,:],\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.08,\n",
    "          callbacks = callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
